{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cee622",
   "metadata": {},
   "source": [
    "# Extracting Contextual Representations from LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b349f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from minicons import cwe\n",
    "from minicons.utils import character_span # for demonstrating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957a7b9",
   "metadata": {},
   "source": [
    "## Cosine similarity calculation setup\n",
    "\n",
    "Below I've defined a function that computes the cosine similarity between every element in tensor A (usually a 2D tensor) to that of every element in tensor B (also usually a 2D tensor).\n",
    "\n",
    "For instance, if A is a n x d matrix and B is a m x d matrix, then the resulting matrix will be n x m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ffa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a: torch.Tensor, b: torch.Tensor, eps =1e-8) -> torch.Tensor:\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sims = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2435ef91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0463,  0.0113, -0.1011, -0.0974],\n",
       "        [ 0.0550,  0.0675, -0.2210, -0.1196],\n",
       "        [-0.1084, -0.3387,  0.2003, -0.0311]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example with random matrices\n",
    "# the d dimension should be same.\n",
    "\n",
    "A = torch.randn(3, 32)\n",
    "B = torch.randn(4, 32)\n",
    "\n",
    "cosine(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5599c6",
   "metadata": {},
   "source": [
    "## Initializing the Model\n",
    "\n",
    "Use the `cwe.CWE` module to initialize an LM. This can be a variety of different models: BERT, RoBERTa, GPT2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d335da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cwe.CWE('bert-base-uncased', device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a8898",
   "metadata": {},
   "source": [
    "## Data formatting\n",
    "The function primarily used for extracting representations from models is `model.extract_representation()`. It accepts batches of instances represented in either of the following formats:\n",
    "\n",
    "```\n",
    "data = [\n",
    "  (sentence_1, word_1),\n",
    "  (sentence_2, word_2),\n",
    "  ....\n",
    "  (sentence_n, word_n)\n",
    "]\n",
    "```\n",
    "or\n",
    "\n",
    "```\n",
    "data = [\n",
    "  (sentence_1, (start_1, end_1)),\n",
    "  (sentence_2, (start_2, end_2)),\n",
    "  ....\n",
    "  (sentence_n, (start_n, end_n))\n",
    "]\n",
    "```\n",
    "where `(start_i, end_i)` are the character span indices for the target word in the ith sentence, i.e., `start_i` is the start index, and `end_i` is the end index.\n",
    "\n",
    "For example, the instance `[\"I like reading books.\", (15, 20)]` corresponds to the word `\"books\"`.\n",
    "\n",
    "Regardless of what is specified, `extract_representation()` reduces the input to the second format. For instance, to get the character span indices of *aircraft* in the first sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f8bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    (\"There is a fair number of bright stars, both single and double, in Lepus.\", \"fair\"),\n",
    "    (\"Using most or all of a work does not bar a finding of fair use.\", \"fair\"),\n",
    "    (\"The rivalry has had its fair share of fights as well.\", \"fair\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b744645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of what a character span looks like. Here, \"fair\" corresponds \n",
    "# to the 11th to 15th character in the string that's in the first \n",
    "# element of the queries object\n",
    "character_span(queries[0][0], 'fair')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8e550",
   "metadata": {},
   "source": [
    "## Extracting representations\n",
    "\n",
    "Below is some code to extract the representations of `fair` from each of the query sentences, from layers 2 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bde9dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_embs = model.extract_representation(queries, layer=[2,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a5e7333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.2002,  1.0760,  0.6308,  ..., -0.6405, -1.1925, -1.4545],\n",
       "         [ 1.2930,  1.4050,  0.1073,  ..., -0.0766, -1.3806, -1.1698],\n",
       "         [ 1.2247,  1.1270,  0.4763,  ..., -0.0422, -1.3762, -1.2325]]),\n",
       " tensor([[ 0.8524,  0.1222,  0.1619,  ..., -0.7295,  0.1747,  0.0786],\n",
       "         [ 0.8999, -0.2127,  0.1966,  ...,  0.2875,  0.1784, -0.4899],\n",
       "         [ 0.6417, -0.6083, -0.1415,  ..., -0.2819, -0.4101, -0.2447]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b729f4",
   "metadata": {},
   "source": [
    "You can also just specify `layer='all'` to get representatiosn from all layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dadcd141",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_embs = model.extract_representation(queries, layer='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad87c7",
   "metadata": {},
   "source": [
    "## Cosine computation\n",
    "\n",
    "Now let's compute the cosine similarities of the representations of \"fair\" across all elements to all other elements in the query. This will give us a 3x3 matrix, but for brevity, I will print the first row. This corresponds to the similarity of \"fair\" in the first sentence to that of all \"fair\" in all sentences (which means the first element will be 1.0, since it is the similarity between two things that are the same). For simplicity, I will compute these values for layers 0, 8, and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f03d9581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: tensor([1.0000, 0.9374, 0.9580])\n",
      "Layer 8: tensor([1.0000, 0.4051, 0.6927])\n",
      "Layer 11: tensor([1.0000, 0.3901, 0.6451])\n"
     ]
    }
   ],
   "source": [
    "for layer in [0, 8, 11]:\n",
    "    first_sim = cosine(layer_embs[layer], layer_embs[layer])[0]\n",
    "    print(f\"Layer {layer}: {first_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac947d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
