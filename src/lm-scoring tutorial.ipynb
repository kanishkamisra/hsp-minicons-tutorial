{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bebaeb6",
   "metadata": {},
   "source": [
    "## Eliciting scores from LMs\n",
    "\n",
    "Language models are trained to predict token probabilities, given some input context. This allows us to explore a number of different scoring methods.\n",
    "\n",
    "* token-scoring: the natural ability of LMs -- assigning probabilities to tokens given context\n",
    "* word-scoring: going from tokens (which could be sub-words) to word scores\n",
    "* sequence-scoring: going from tokens/words to full, multi-word sequences \n",
    "* conditional-scoring: computing conditional probabilities of sequences given some input\n",
    "\n",
    "For all these methods, we will consider a range of different scores: probabilities, log-probabilities, surprisals. In the context of sequence probabilities, we will look at differences between summing log-probabilities (equivalent to multiplying probabilities) vs. looking at log-probability per token, to account for the effect of length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eab885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanishka/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from minicons import cwe, scorer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419b5bd",
   "metadata": {},
   "source": [
    "## Contextualized word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d42b482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a: torch.Tensor, b: torch.Tensor, eps =1e-8) -> torch.Tensor:\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sims = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85769934",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = cwe.CWE(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae5dba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    (\"I saw a bat flying around looking for food.\", \"bat\"),\n",
    "    (\"I think bats are fierce when they are hungry.\", \"bats\"),\n",
    "    (\"He swung his bat at me but I dodged it.\", \"bat\")\n",
    "]\n",
    "\n",
    "layer_embs = lm.extract_representation(queries, layer='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed3cb919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7299, 0.6701],\n",
       "        [0.7299, 1.0000, 0.5523],\n",
       "        [0.6701, 0.5523, 1.0000]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(layer_embs[11], layer_embs[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6c43b",
   "metadata": {},
   "source": [
    "### Different types of LMs\n",
    "\n",
    "Autoregressive LMs: `lm.IncrementalLMScorer`\n",
    "\n",
    "Masked LMs: `lm.MaskedLMScorer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eebd2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "# model_name = \"gpt2\"\n",
    "# model_name = \"facebook/opt-125m\"\n",
    "\n",
    "# many models do not automatically insert a beggining of \n",
    "# sentence tokens when tokenizing a sequence, even though\n",
    "# they were trained to do so...\n",
    "\n",
    "if \"gpt2\" in model_name or \"pythia\" in model_name or \"SmolLM\" in model_name:\n",
    "    BOS = True\n",
    "else:\n",
    "    BOS = False\n",
    "\n",
    "lm = scorer.IncrementalLMScorer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80264f25",
   "metadata": {},
   "source": [
    "### Token Scoring\n",
    "\n",
    "**Input:** I know what the lion devoured at sunrise.\n",
    "\n",
    "**Outputs:** \n",
    "* Probabilities: $p(w_i | w_1, w_2, \\dots, w_{i-1})$\n",
    "* log-probabilities: $\\log p(w_i | w_1, w_2, \\dots, w_{i-1})$\n",
    "* Surprisals: $-\\log p(w_i | w_1, w_2, \\dots, w_{i-1})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03e44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I know what the lion devoured at sunrise.\", \n",
    "    \"I know that the lion devoured at sunrise.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7f83f",
   "metadata": {},
   "source": [
    "Probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e56aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<|endoftext|>', 0.0),\n",
       "  ('I', 0.002043517306447029),\n",
       "  ('Ġknow', 0.017165811732411385),\n",
       "  ('Ġwhat', 0.0534108504652977),\n",
       "  ('Ġthe', 0.0762874037027359),\n",
       "  ('Ġlion', 5.0759124860633165e-05),\n",
       "  ('Ġdev', 9.478997526457533e-06),\n",
       "  ('oured', 0.10184398293495178),\n",
       "  ('Ġat', 0.021646004170179367),\n",
       "  ('Ġsunrise', 0.0006119301542639732),\n",
       "  ('.', 0.1724427342414856)],\n",
       " [('<|endoftext|>', 0.0),\n",
       "  ('I', 0.002043517306447029),\n",
       "  ('Ġknow', 0.017165811732411385),\n",
       "  ('Ġthat', 0.2915889024734497),\n",
       "  ('Ġthe', 0.13011622428894043),\n",
       "  ('Ġlion', 8.081334817688912e-05),\n",
       "  ('Ġdev', 7.503035885747522e-05),\n",
       "  ('oured', 0.32274961471557617),\n",
       "  ('Ġat', 0.0006744182901456952),\n",
       "  ('Ġsunrise', 8.538211841369048e-05),\n",
       "  ('.', 0.07048003375530243)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.token_score(\n",
    "    sequences, \n",
    "    bos_token=BOS,\n",
    "    prob=True,\n",
    "    bow_correction=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3314f66",
   "metadata": {},
   "source": [
    "log probabilities/surprisals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3c4660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<|endoftext|>', 0.0),\n",
       "  ('I', 6.193082809448242),\n",
       "  ('Ġknow', 4.064835548400879),\n",
       "  ('Ġwhat', 2.929741382598877),\n",
       "  ('Ġthe', 2.5732474327087402),\n",
       "  ('Ġlion', 9.888419151306152),\n",
       "  ('Ġdev', 11.566431999206543),\n",
       "  ('oured', 2.284313201904297),\n",
       "  ('Ġat', 3.8329343795776367),\n",
       "  ('Ġsunrise', 7.398892402648926),\n",
       "  ('.', 1.7576900720596313)],\n",
       " [('<|endoftext|>', 0.0),\n",
       "  ('I', 6.193082809448242),\n",
       "  ('Ġknow', 4.064835548400879),\n",
       "  ('Ġthat', 1.2324103116989136),\n",
       "  ('Ġthe', 2.0393271446228027),\n",
       "  ('Ġlion', 9.423368453979492),\n",
       "  ('Ġdev', 9.497617721557617),\n",
       "  ('oured', 1.1308784484863281),\n",
       "  ('Ġat', 7.301660060882568),\n",
       "  ('Ġsunrise', 9.36837387084961),\n",
       "  ('.', 2.652425765991211)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.token_score(\n",
    "    sequences, \n",
    "    bos_token=BOS,\n",
    "    surprisal=True,\n",
    "    bow_correction=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0adc1",
   "metadata": {},
   "source": [
    "### Word scoring\n",
    "\n",
    "Same metrics, but logprobs for words that are split into tokens are summed---e.g., `devoured` is split into `dev + oured`. However, here you have to provide the word tokenizer yourself. We will use `nltk`'s `TweetTokenizer()` as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddc7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = TweetTokenizer().tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe5085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 8.93472957611084),\n",
       "  ('know', 5.864317893981934),\n",
       "  ('what', 4.2267231941223145),\n",
       "  ('the', 3.712411403656006),\n",
       "  ('lion', 14.265973091125488),\n",
       "  ('devoured', 19.98240089416504),\n",
       "  ('at', 5.529755592346191),\n",
       "  ('sunrise', 10.674345016479492),\n",
       "  ('.', 2.535810708999634)],\n",
       " [('I', 8.93472957611084),\n",
       "  ('know', 5.864317893981934),\n",
       "  ('that', 1.7779922485351562),\n",
       "  ('the', 2.942127227783203),\n",
       "  ('lion', 13.595046997070312),\n",
       "  ('devoured', 15.333678245544434),\n",
       "  ('at', 10.534069061279297),\n",
       "  ('sunrise', 13.515706062316895),\n",
       "  ('.', 3.82664155960083)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.word_score_tokenized(\n",
    "    sequences, \n",
    "    bos_token=BOS, \n",
    "    tokenize_function=word_tokenizer,\n",
    "    surprisal=True,\n",
    "    base_two=True,\n",
    "    bow_correction=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dfea9",
   "metadata": {},
   "source": [
    "### Sequence scoring\n",
    "\n",
    "**Input:** batch of sentences\n",
    "\n",
    "**Outputs:** scores indicating how likely each sequence is. There are multiple methods for doing this though:\n",
    "\n",
    "* summed log-probs (equivalent to joint probability, computed using the product rule)\n",
    "* log-prob per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9fed358",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"The keys to the cabinet are on the table.\",\n",
    "    \"The keys to the cabinet is on the table.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cde6ee",
   "metadata": {},
   "source": [
    "log-prob per token (default behavior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "835d41ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.7346298694610596, -4.106328010559082]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.sequence_score(sequences, bos_token=BOS, bow_correction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda2135",
   "metadata": {},
   "source": [
    "summed log-probs:\n",
    "\n",
    "summing is done by using the `reduction` argument, which takes a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9453233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-37.34629821777344, -41.06328201293945]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.sequence_score(\n",
    "    sequences, \n",
    "    bos_token=BOS, \n",
    "    bow_correction=True,\n",
    "    reduction=lambda x: x.sum().item()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ddd0f",
   "metadata": {},
   "source": [
    "Here, the lambda function is a concise way of defining a function, here this is equivalent to taking the torch tensor consisting of the model elicited log-probabilities and reduces it row-wise by summing, and extracting the item (as opposed to keeping it as a `tensor`). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1165f7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.030834"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0.223234, 0.443257, 0.364343], dtype=torch.double)\n",
    "sum_func = lambda x: x.sum().item()\n",
    "\n",
    "sum_func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10583d",
   "metadata": {},
   "source": [
    "### log-prob of full sequence (summing) vs. log-prob per token (avg)\n",
    "\n",
    "Usually, the two metrics show similar qualitative trends, especially for minimal pair comparisons. However there are certain cases where log-prob per token is a better metric. This is because the summed log prob metric for a sentence might be lower simply because it is longer (contain more tokens)--since it involves a more number of multiplications between word-probabilities, each of which is a number lower than 1.\n",
    "\n",
    "The following pair illustrates this issue:\n",
    "\n",
    "1. These casseroles disgust Mrs. O'leary\n",
    "2. *These casseroles disgusts Kayla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "277629e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    # longer but grammatical\n",
    "    \"These casseroles disgust Mrs. O'leary\",\n",
    "    # shorter but ungrammatical\n",
    "    \"These casseroles disgusts Kayla\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c0704c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-62.72190856933594, -56.759830474853516]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum\n",
    "lm.sequence_score(\n",
    "    stimuli, \n",
    "    bos_token=BOS, \n",
    "    bow_correction=True,\n",
    "    reduction=lambda x: x.sum().item()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a3c1f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-6.272191047668457, -8.10854721069336]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.sequence_score(stimuli, bos_token=BOS, bow_correction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e5ff9",
   "metadata": {},
   "source": [
    "### Conditional LM scoring\n",
    "\n",
    "This follows the same principle as sequence scoring, but allows you to separate the prefix and the continuation. Like sequence scoring, this method also allows for different reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74e25b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lake and Murphy (2023) / Murphy (1988)\n",
    "# \"are cooked in a pie\" is more salient to sliced apples\n",
    "# than to \"apples\" or \"sliced things\"\n",
    "\n",
    "prefixes = [\"Sliced apples\", \"Apples\", \"Sliced things\"]\n",
    "continuations = [\"are cooked in a pie\"] * 3\n",
    "\n",
    "# log P(continuation | prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edb7fbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.195496082305908, -3.9255783557891846, -3.6544346809387207]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.conditional_score(prefixes, continuations, bos_token=BOS, bow_correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6742f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
